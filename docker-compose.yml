services:
  airflow:
    build:
      context: ./containers/airflow
    env_file: ./containers/airflow/airflow.env
    volumes:
      - ./dags:/opt/airflow/dags
      - ./jobs:/jobs
      - ./logs/airflow:/opt/airflow/logs
    ports:
      - "8080:8080"

  spark-master:
    image: bitnami/spark:3.5
    env_file: ./containers/spark/spark-master.env
    volumes:
      - ./containers/spark/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
    ports:
      - 8081:8080

  spark-worker:
    image: bitnami/spark:3.5
    env_file: ./containers/spark/spark-worker.env
    volumes:
      - ./containers/spark/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf

  namenode:
    image: apache/hadoop:3
    hostname: namenode
    command: ["hdfs", "namenode"]
    ports:
      - 9870:9870
    env_file:
      - ./containers/hdfs/hdfs.env
    environment:
      ENSURE_NAMENODE_DIR: "/tmp/hadoop-root/dfs/name"
  
  datanode:
    image: apache/hadoop:3
    command: ["hdfs", "datanode"]
    env_file:
      - ./containers/hdfs/hdfs.env

  hive-metastore:
    image: apache/hive:3.1.3
    hostname: hive-metastore
    depends_on:
      - namenode
    environment:
      - HIVE_CUSTOM_CONF_DIR=/hive-custom-conf
      - SERVICE_NAME=metastore
    volumes:
      - ./containers/hive-metastore:/hive-custom-conf